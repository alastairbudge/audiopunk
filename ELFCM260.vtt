WEBVTT

NOTE
This file was generated by Descript <www.descript.com>

00:00:05.034 --> 00:00:12.124
Hello, hello hello, and welcome to English Learning for Curious Minds, by Leonardo English.

00:00:12.434 --> 00:00:17.444
The show where you can listen to fascinating stories, and learn weird and

00:00:17.454 --> 00:00:21.734
wonderful things about the world at the same time as improving your English.

00:00:22.374 --> 00:00:28.504
I'm Alastair Budge, and today we are going to be talking about The Cambridge Analytica Scandal.

00:00:29.164 --> 00:00:36.484
It’s a story that brings together the power of Facebook, journalism, political prejudice, right vs.

00:00:36.524 --> 00:00:43.304
left, democracy, human psychology, free will, allegations of Russian spies working

00:00:43.304 --> 00:00:48.834
to influence foreign elections and a plot to put Donald Trump in the White House.

00:00:49.344 --> 00:00:50.454
The only problem is.

00:00:50.994 --> 00:00:52.914
Is any of it even true?

00:00:54.857 --> 00:00:59.127
Right, let’s get started and talk about Cambridge Analytica.

00:01:00.167 --> 00:01:07.157
If you had bought the Guardian Newspaper on March 18th of 2018, on the front

00:01:07.207 --> 00:01:14.067
cover you would have seen a picture of a 28-year-old man with short, pink hair.

00:01:14.947 --> 00:01:18.187
Next to him, the all-powerful headline:

00:01:18.777 --> 00:01:25.327
“Revealed: 50 million Facebook files taken in record data breach.”

00:01:25.917 --> 00:01:30.057
A breach, by the way, means a break, a hole that has

00:01:30.057 --> 00:01:33.557
been made in order to get in and access something.

00:01:34.687 --> 00:01:41.107
The story went on to reveal that a company called Cambridge Analytica had used the personal

00:01:41.157 --> 00:01:48.937
Facebook data of 50 million people to target US citizens, build a–and I’m quoting directly

00:01:48.937 --> 00:01:55.687
here–”psychological warfare tool” which targeted people based on their stolen data, and

00:01:55.867 --> 00:02:03.777
ultimately swing the election, to win the 2016 US presidential election for Donald Trump.

00:02:04.657 --> 00:02:08.757
It was a powerful story, and it was all over the news.

00:02:09.167 --> 00:02:11.267
You may well remember the story yourself.

00:02:12.017 --> 00:02:17.247
It created big problems for Facebook, and the company’s share price.

00:02:17.707 --> 00:02:23.447
It lost $35 billion in value within a day, and in the subsequent

00:02:23.447 --> 00:02:28.057
months it lost over $100 billion in stock market value.

00:02:29.137 --> 00:02:34.967
The company behind the supposed data breach, Cambridge Analytica, went out of

00:02:34.977 --> 00:02:42.057
business, and its former CEO was forced to testify in front of the British parliament.

00:02:43.237 --> 00:02:49.387
The journalist behind the story, Carole Cadwalladr, was invited to give a TED talk,

00:02:49.717 --> 00:02:55.707
and won the prestigious George Orwell prize for journalism for breaking this story.

00:02:56.347 --> 00:03:02.547
So, what we are going to try to answer in this episode is what actually happened?

00:03:03.087 --> 00:03:11.047
Is democracy really at threat from bad actors using personal data to influence our voting behaviour?

00:03:11.467 --> 00:03:15.117
Or is the entire story completely overblown?

00:03:15.967 --> 00:03:18.397
So, let’s start with what happened.

00:03:19.247 --> 00:03:23.097
The main character in the story, without whom none of this

00:03:23.157 --> 00:03:26.727
would have been possible, was a man called Aleksandr Kogan.

00:03:27.577 --> 00:03:31.707
He was born in the USSR, the former Soviet Union, but

00:03:31.877 --> 00:03:35.107
moved to the United States when he was 7 years old.

00:03:35.687 --> 00:03:42.067
Kogan excelled at school, and showed a remarkable talent for mathematics and physics.

00:03:43.187 --> 00:03:49.257
At university he became increasingly interested in psychology, in understanding

00:03:49.307 --> 00:03:55.147
why humans feel certain emotions, and what makes us behave in the way we do.

00:03:56.517 --> 00:04:02.587
He had been working at Cambridge University since 2012, and saw from early

00:04:02.607 --> 00:04:08.777
on the potential power that a then newish technology company, Facebook, had.

00:04:09.847 --> 00:04:15.187
If you used Facebook back in 2012, I imagine it played a bigger,

00:04:15.307 --> 00:04:19.357
or at least more obvious, role in your life than it does now.

00:04:20.097 --> 00:04:24.887
You used that “like” button to like anything from a status update

00:04:24.917 --> 00:04:28.797
from a friend to a “raising money for dog shelters” campaign.

00:04:29.647 --> 00:04:35.637
You probably didn’t think too much about what happened after you clicked that “Like” button.

00:04:36.207 --> 00:04:40.917
After all, Facebook was a fun way to keep in touch with friends and

00:04:40.917 --> 00:04:45.937
family, share photos and generally see what was going on in the world.

00:04:46.927 --> 00:04:52.907
To Kogan, however, it seemed like the most incredible dataset on humanity.

00:04:53.947 --> 00:04:58.397
People like you and me were spending all this time on Facebook, liking

00:04:58.397 --> 00:05:03.097
stuff, interacting with it, giving Facebook data on what we like and

00:05:03.097 --> 00:05:07.077
what we don’t like, what we engage with and what we aren’t interested in.

00:05:08.387 --> 00:05:13.517
What’s more, Facebook could see who all of our friends were, so it had

00:05:13.517 --> 00:05:17.897
this amazing understanding of connections between all of its users.

00:05:18.947 --> 00:05:25.357
In 2014, Facebook passed 1.3 billion users, each of whom

00:05:25.517 --> 00:05:29.457
was spending an average of 40 minutes a day on the network.

00:05:30.167 --> 00:05:36.087
So not only did it have a vast amount of people using it, but they were using it a lot.

00:05:37.157 --> 00:05:40.287
To someone like Kogan, this was fascinating.

00:05:41.397 --> 00:05:45.687
What’s more, at the time, Facebook had recently started to allow

00:05:45.887 --> 00:05:50.767
third-parties to develop applications on top of Facebook that would

00:05:50.777 --> 00:05:55.247
give them certain permissions to view the data on Facebook users.

00:05:56.317 --> 00:06:01.387
If you can think back to this time, if you were a Facebook user that is, you might

00:06:01.417 --> 00:06:06.177
remember a load of quizzes where you granted the quiz access to your Facebook

00:06:06.177 --> 00:06:11.217
account, answered a few questions, and it would tell you things like what Harry

00:06:11.217 --> 00:06:15.737
Potter character you were like or what member of the Beatles you would have been.

00:06:16.367 --> 00:06:18.387
Silly stuff, harmless fun.

00:06:19.097 --> 00:06:20.847
Or so most people thought.

00:06:21.577 --> 00:06:27.097
Indeed, back in 2014 these third-party applications were able to

00:06:27.097 --> 00:06:31.827
access some data not just about you but also about your friends.

00:06:32.577 --> 00:06:37.387
Someone might give access to a quiz to find out what type of pizza they are,

00:06:37.657 --> 00:06:43.177
agreeing to share their data with the app, but not realise that they were actually

00:06:43.207 --> 00:06:47.747
allowing the application to access information about their friends as well.

00:06:49.297 --> 00:06:54.017
Kogan’s question was, was any of this Facebook data actually any

00:06:54.037 --> 00:06:58.537
good at telling you anything useful about that person’s personality?

00:06:59.517 --> 00:07:05.308
If someone on Facebook liked “dogs, Mars bars and reading”, did

00:07:05.308 --> 00:07:09.118
this actually tell you anything about them as an individual?

00:07:10.288 --> 00:07:15.548
But Kogan was no thief, and he couldn’t just take this data from Facebook.

00:07:16.118 --> 00:07:20.098
Facebook users needed to give him their permission to use it.

00:07:20.878 --> 00:07:24.079
And it’s here that we meet Cambridge Analytica.

00:07:24.708 --> 00:07:29.068
Cambridge Analytica, which has no links whatsoever to Cambridge

00:07:29.068 --> 00:07:34.821
University, by the way, was a political consulting company founded in 2013.

00:07:35.628 --> 00:07:42.938
Its supposed speciality was advising political campaigns, advising politicians on how to use

00:07:42.968 --> 00:07:49.188
social media and data to put the right message in front of the right people at the right time.

00:07:50.228 --> 00:07:56.788
Cambridge Analytica paid Kogan to create an app that would collect data on Facebook users.

00:07:57.588 --> 00:08:02.658
Cambridge Analytica would be able to use the data for use with its political

00:08:02.668 --> 00:08:08.308
clients, and Kogan could continue his academic research with the data he collected.

00:08:08.958 --> 00:08:13.118
A win-win situation, one where both parties benefited.

00:08:13.868 --> 00:08:18.238
So, Kogan, with Facebook’s permission, built a simple

00:08:18.268 --> 00:08:21.258
application called “This Is Your Digital Life”.

00:08:21.788 --> 00:08:23.458
It was a quiz, essentially.

00:08:23.998 --> 00:08:29.018
Users were offered three or four dollars to participate in the survey, and

00:08:29.238 --> 00:08:34.148
in exchange they gave the app permissions to access their Facebook data.

00:08:35.198 --> 00:08:38.988
What they probably didn’t realise, though, was that they were actually

00:08:39.128 --> 00:08:43.058
giving the app permission to see some data on all of their friends.

00:08:43.408 --> 00:08:50.028
So if I authorised the app, if I took the quiz, the app would get data on all of my friends.

00:08:50.578 --> 00:08:56.661
If I had 500 friends, for example, the app got the data on those 500 friends.

00:08:58.138 --> 00:09:03.648
The quiz was taken by around 300,000 people, each one with hundreds of

00:09:03.648 --> 00:09:09.408
friends, meaning that Kogan now had Facebook data on almost 90 million people.

00:09:10.088 --> 00:09:14.888
The initial Guardian report said 50 million, but Facebook was later

00:09:14.888 --> 00:09:18.678
forced to admit that it had actually been almost double this number.

00:09:19.628 --> 00:09:26.058
Getting this data wasn’t cheap, as the app had to incentivise people to take the personality test.

00:09:27.168 --> 00:09:32.818
Cambridge Analytica provided the financing for it, about a million dollars, believing

00:09:32.828 --> 00:09:37.638
that the data that would come out at the end of it would be significantly more valuable.

00:09:38.598 --> 00:09:43.848
Now, let’s just pause to address a couple of points first, before we move on with the story.

00:09:44.768 --> 00:09:48.768
Kogan had Facebook’s permission to create the quiz application.

00:09:48.988 --> 00:09:50.308
There’s no dispute about that.

00:09:51.378 --> 00:09:57.854
What is disputed is that Kogan says he was given permission to sell this data on to third parties.

00:09:58.858 --> 00:10:00.428
Facebook says he wasn’t.

00:10:01.358 --> 00:10:06.708
Where it does become a little blurry, a little less clear, is about how

00:10:06.718 --> 00:10:12.038
the data on Facebook users was accessed, and the use of the term “breach”.

00:10:12.918 --> 00:10:17.708
As a reminder, a breach is when illegal access is provided to something.

00:10:18.488 --> 00:10:23.428
At the time, Facebook’s default settings meant that if you gave an app

00:10:23.498 --> 00:10:28.278
permission to your data, by default it could get data on all of your friends.

00:10:29.318 --> 00:10:35.668
The Facebook users likely didn’t understand exactly what they were giving their permission for, but

00:10:35.988 --> 00:10:42.898
Kogan’s app was one of hundreds, thousands even, that was collecting very similar amounts of data.

00:10:43.598 --> 00:10:48.088
So Kogan wasn’t doing anything illegal, and indeed he was

00:10:48.248 --> 00:10:51.268
doing what practically every other Facebook app was doing.

00:10:52.538 --> 00:10:57.708
Ok, so with those clarifications out of the way, let’s return to our story.

00:10:58.738 --> 00:11:03.198
You might be thinking, what is Kogan actually getting out of all of this?

00:11:03.538 --> 00:11:09.338
Was he really a Russian agent, was he a closet Trump supporter, did he want to wreak

00:11:09.348 --> 00:11:15.048
havoc in foreign elections and provide data that could be used to undermine democracy?

00:11:15.908 --> 00:11:20.388
That would make for a good story, but unfortunately it doesn’t seem to be true.

00:11:21.628 --> 00:11:26.688
He says that he was unaware that the data would be used for political targeting,

00:11:26.938 --> 00:11:32.628
and his interest in doing all of this was primarily from a research perspective.

00:11:33.538 --> 00:11:39.218
He wanted to know whether all of this data he was collecting on people - what they liked, where they

00:11:39.218 --> 00:11:45.418
lived, what they did, whether this data actually helped predict anything about their personalities.

00:11:46.328 --> 00:11:52.008
Remember, he is an amazingly talented scientist with an interest in human psychology.

00:11:52.328 --> 00:11:57.108
He wanted to know how human behaviour and personality can be predicted.

00:11:58.098 --> 00:12:03.296
And as for Cambridge Analytica, if Kogan’s work proved to be able to

00:12:03.296 --> 00:12:08.076
predict human behaviour and personality, it would be amazingly valuable.

00:12:08.846 --> 00:12:13.786
Imagine that just by knowing that if someone liked Mars bars, dogs and

00:12:13.786 --> 00:12:18.076
reading they would be particularly receptive to a certain type of political

00:12:18.106 --> 00:12:23.811
message, and if someone liked singing, learning languages and had a birthday

00:12:23.811 --> 00:12:28.661
in December then they would be receptive to another type of political message.

00:12:29.401 --> 00:12:34.281
Armed with this information on people, and with Facebook, the ability

00:12:34.281 --> 00:12:38.581
to target over a billion people, and the vast majority of the voters

00:12:38.701 --> 00:12:43.131
in the United States, this would be an incredibly powerful weapon.

00:12:44.151 --> 00:12:49.061
But how could you actually figure out whether the data was useful or not?

00:12:49.901 --> 00:12:53.231
Well, that’s where the personality quiz came in.

00:12:54.261 --> 00:13:00.181
After someone gave access to Kogan’s application, they needed to fill out the personality quiz.

00:13:01.161 --> 00:13:05.611
Sure, you can say that people aren’t very good at assessing their own personality,

00:13:05.831 --> 00:13:10.261
but as there was no reason for someone to fill in a question incorrectly,

00:13:10.541 --> 00:13:16.421
and the test was completely randomised, this data should have been accurate.

00:13:17.491 --> 00:13:23.101
After the Facebook user had completed the quiz, Kogan’s algorithm looked at their

00:13:23.101 --> 00:13:28.731
answers and classified them by five different personality traits - such as if

00:13:28.731 --> 00:13:33.701
they were an extrovert or introvert, how open they were to new ideas, and so on.

00:13:34.931 --> 00:13:39.101
It was then time to see whether all the data he had collected from

00:13:39.121 --> 00:13:43.751
Facebook was actually any good at predicting these people’s personality.

00:13:44.161 --> 00:13:45.561
It was crunch time.

00:13:46.611 --> 00:13:50.811
Unfortunately, according to Kogan, the algorithms managed

00:13:50.811 --> 00:13:54.971
to correctly predict someone’s personality 1% of the time.

00:13:55.431 --> 00:14:03.171
That is 1%, 1 one out of 100 times, meaning that 99% of the time it got it wrong.

00:14:04.411 --> 00:14:10.331
The Facebook data simply didn’t seem to be any good at all at predicting someone’s personality.

00:14:11.231 --> 00:14:14.791
Now, you might have thought that this would be the end of the story,

00:14:14.991 --> 00:14:19.674
with Cambridge Analytica cutting its losses as the data was junk.

00:14:20.464 --> 00:14:22.374
But really it was just the start.

00:14:23.834 --> 00:14:29.524
Cambridge Analytica seemed utterly uninterested in how accurate the data actually was.

00:14:30.034 --> 00:14:34.624
Instead, it continued to tell its clients that it had an incredibly

00:14:34.634 --> 00:14:38.444
valuable dataset that could be used for political targeting.

00:14:39.194 --> 00:14:46.934
To be precise, it said it had up to 5,000 data points on over 220 million Americans.

00:14:48.064 --> 00:14:51.134
The company also had connections in high places.

00:14:51.434 --> 00:14:56.904
Its investors included the billionaire Republican donor, Robert Mercer, and the

00:14:56.904 --> 00:15:02.124
man who would later go on to become Donald Trump’s chief strategist, Steve Bannon.

00:15:03.364 --> 00:15:08.264
These powerful connections would give it a head start in the US political scene,

00:15:08.584 --> 00:15:14.154
and would later result in it working on the 2016 US presidential campaign.

00:15:15.274 --> 00:15:19.924
While it would be most famous for working with Donald Trump, its first client

00:15:20.094 --> 00:15:25.414
was actually his competitor, the Republican establishment candidate, Ted Cruz.

00:15:26.624 --> 00:15:31.374
Cambridge Analytica had managed to sell Cruz on the power of its data

00:15:31.504 --> 00:15:37.064
and analytics capabilities, but as you will know, Cruz was heavily beaten

00:15:37.334 --> 00:15:41.184
by Trump in the nomination to be the Republican candidate for president.

00:15:42.434 --> 00:15:48.504
Sure, you might think, data can’t compensate for an uninspiring candidate, and

00:15:48.754 --> 00:15:52.944
there was only a limited amount that Cambridge Analytica could have done for Cruz.

00:15:53.854 --> 00:15:55.554
In any case, it didn’t work.

00:15:56.304 --> 00:15:59.994
But that didn’t stop the company from being employed by the

00:15:59.994 --> 00:16:05.304
eventually victorious anti-establishment Trump campaign in 2016.

00:16:06.274 --> 00:16:11.044
It later emerged that Cambridge Analytica had told potential clients that it

00:16:11.064 --> 00:16:16.754
had also consulted on the Brexit campaign, the campaign to leave the EU in

00:16:16.754 --> 00:16:22.284
June of 2016, which happened six months before Trump’s victory in November.

00:16:23.274 --> 00:16:30.234
And the CEO of Cambridge Analytica, a tall man named Alexander Nix with an almost Bond-villain

00:16:30.234 --> 00:16:36.504
style, would present at conferences and give interviews where he talked about the power of the

00:16:36.504 --> 00:16:44.144
company’s data and analytics capabilities, boasting about how many academics and PhDs it employed,

00:16:44.264 --> 00:16:50.918
and how it was able to harness the power of big data to change public opinion about anything.

00:16:52.298 --> 00:16:56.518
Then, in March of 2018, the scandal broke.

00:16:57.198 --> 00:17:01.048
It was front-page news in The Guardian and the New York Times,

00:17:01.108 --> 00:17:05.178
and was all over cable news around the world the very same day.

00:17:06.108 --> 00:17:11.458
For everyone who wasn’t involved in the world of political campaigning, so for

00:17:11.468 --> 00:17:16.238
almost everyone, it was the first time they had heard the name Cambridge Analytica.

00:17:16.948 --> 00:17:18.288
And it was frightening.

00:17:18.678 --> 00:17:23.878
Experts were invited to come on TV and talk about this threat to democracy.

00:17:24.398 --> 00:17:28.498
Could free will continue to exist if companies were able to

00:17:28.498 --> 00:17:32.568
predict and change our voting patterns without our knowledge?

00:17:33.748 --> 00:17:37.308
For many, it was the first time that they had thought about the

00:17:37.348 --> 00:17:41.368
power and possibilities of the data that Facebook had on them.

00:17:42.258 --> 00:17:48.918
The hashtag #DeleteFacebook was trending on Twitter, and Mark Zuckerberg was dragged to

00:17:48.928 --> 00:17:54.748
testify before the US Congress while his company was losing billions of dollars of value.

00:17:55.618 --> 00:17:58.088
It made for a wonderful story.

00:17:58.698 --> 00:18:02.178
To people who had questioned how a country could possibly have

00:18:02.188 --> 00:18:06.478
voted for Brexit or Donald Trump, it provided an explanation.

00:18:07.718 --> 00:18:11.228
The people had been tricked, psychologically profiled

00:18:11.278 --> 00:18:14.428
and targeted with messages to influence their opinion.

00:18:15.538 --> 00:18:19.028
There was even the fact that Alexander Kogan was born in

00:18:19.028 --> 00:18:23.438
Russia, which to some was a sign that he was a Russian spy.

00:18:24.008 --> 00:18:28.358
Journalists called him up and asked him point-blank whether he was a Russian agent.

00:18:29.118 --> 00:18:32.508
Suddenly it was clear, and everything made sense.

00:18:33.578 --> 00:18:38.238
But the reality is that the scandal was a bit of a storm in a

00:18:38.258 --> 00:18:42.518
teacup, it was a scandal over something that didn’t really happen.

00:18:43.438 --> 00:18:49.848
Kogan, the man who collected the data and ran the statistical models, has said numerous times

00:18:50.028 --> 00:18:55.958
that the data was practically useless on a personal level, even when it was first collected.

00:18:57.238 --> 00:19:04.638
What’s more, it was collected in 2014 and would have been pretty out of date in 2016 anyway.

00:19:05.088 --> 00:19:10.598
Data like this typically has a shelf-life of a maximum of 12 months.

00:19:11.328 --> 00:19:17.298
Plus, the fact that the Cruz campaign and the Trump campaign both got rid of Cambridge

00:19:17.308 --> 00:19:24.328
Analytica suggests that the people who paid to use the data, and supposedly used it to target

00:19:24.328 --> 00:19:30.148
voters with hyper-targeted advertising and swing an election, didn’t find it useful at all.

00:19:31.258 --> 00:19:36.528
And as far as the question of whether the data had any impact on the Brexit

00:19:36.558 --> 00:19:41.098
vote, there was an extensive enquiry by the UK Information Commissioner’s

00:19:41.138 --> 00:19:47.058
Office, essentially an arm of government, which found absolutely no evidence that

00:19:47.058 --> 00:19:52.208
Facebook data held by Cambridge Analytica had any influence on the Brexit vote.

00:19:53.158 --> 00:19:57.948
So, let’s recap quickly, as there are two slightly different questions:

00:19:58.758 --> 00:20:02.738
Firstly, was there an illegal breach of Facebook data?

00:20:03.278 --> 00:20:07.048
And secondly, did this data actually influence anything?

00:20:08.488 --> 00:20:14.198
To the first question, no, it wasn’t illegal, but it was probably misleading.

00:20:14.898 --> 00:20:19.578
The users who authorised this application probably didn’t know that they were

00:20:19.618 --> 00:20:24.738
also giving their friends’ data, or that it might be used for commercial purposes.

00:20:25.668 --> 00:20:32.498
As a result, Facebook has clarified its policies on this, and third-party applications don’t

00:20:32.498 --> 00:20:38.248
get access to your friends’ data, and they get access to much less data than they used to get.

00:20:39.098 --> 00:20:43.608
If you authorise a third-party app now on Facebook, it’s a lot

00:20:43.668 --> 00:20:48.158
clearer, and these apps don’t tend to get access to very much.

00:20:49.328 --> 00:20:56.168
And to our second question, did the data actually influence anything, although it makes for a much

00:20:56.358 --> 00:21:03.168
juicier and more interesting story if you think it does, the reality certainly appears to be “no”.

00:21:04.388 --> 00:21:09.028
Now, as a final point, just in case this is mistaken for some great

00:21:09.038 --> 00:21:13.648
defence of Facebook or Cambridge Analytica, it most definitely isn’t.

00:21:14.468 --> 00:21:19.728
The idea that you have accidentally given access to your personality, what you like

00:21:19.748 --> 00:21:25.948
and dislike, who you are, the idea that some evil third-party knows it is scary.

00:21:26.378 --> 00:21:30.398
It’s not nice, and it’s definitely not nice to think that you might

00:21:30.408 --> 00:21:34.548
have let this third-party have access to your nearest and dearest.

00:21:35.418 --> 00:21:39.948
But the reality is that this data, at least in the case of the Cambridge

00:21:39.968 --> 00:21:44.408
Analytica scandal, is not nearly as useful as you might think it to be.

00:21:45.368 --> 00:21:51.108
Indeed, there is one theory that the Cambridge Analytica scandal was actually excellent

00:21:51.118 --> 00:21:56.828
news for Facebook, because it made the world think that the data Facebook had on its

00:21:56.828 --> 00:22:02.938
users was significantly more valuable than Kogan’s analysis suggested it actually was.

00:22:03.778 --> 00:22:09.508
Now, to go back to our example of someone who likes “dogs, Mars bars and reading”.

00:22:10.258 --> 00:22:12.708
Well, I like “dogs, Mars bars and reading”.

00:22:13.318 --> 00:22:16.848
Maybe you like “dogs, Mars bars and reading” too.

00:22:17.428 --> 00:22:22.778
Does this mean that we’re similar, and most importantly, do these three things

00:22:22.928 --> 00:22:27.108
mean that we are likely to have the same political beliefs, care about the

00:22:27.108 --> 00:22:31.918
same issues, and are likely to respond in the same way to a political advert?

00:22:32.488 --> 00:22:39.278
It’s a possibility, but from what Kogan’s research suggests, it certainly seems pretty unlikely.

00:22:40.418 --> 00:22:43.478
This is, of course, not to say that the information

00:22:43.658 --> 00:22:47.018
Facebook holds on you or me is worthless and unusable.

00:22:47.648 --> 00:22:51.928
Facebook knows a lot more about us than whether we like dogs, Mars

00:22:51.928 --> 00:22:56.268
bars and reading, and hundreds of millions of dollars are spent every

00:22:56.268 --> 00:23:00.698
single day advertising to users on Facebook because of what Facebook

00:23:00.748 --> 00:23:02.488
knows about them.

00:23:02.488 --> 00:23:03.648
But can

00:23:03.698 --> 00:23:07.638
this data be easily stolen by a third party, or even

00:23:07.638 --> 00:23:11.288
a foreign agent, and used to manipulate democracy?

00:23:12.157 --> 00:23:18.287
Well, with the example of Cambridge Analytica, it certainly

00:23:18.287 --> 00:23:20.067
seems that even when we think it has, it really hasn't.

00:23:22.307 --> 00:23:27.627
OK then, that is it for today's episode on the Cambridge Analytica Scandal.

00:23:28.017 --> 00:23:32.627
I hope it's been an interesting one, that you've learnt something new, and if you remember

00:23:32.627 --> 00:23:38.107
this scandal from a few years ago, well perhaps it has put a different perspective on it.

00:23:39.027 --> 00:23:42.467
As always, I would love to know what you thought of this episode.

00:23:42.747 --> 00:23:44.807
Do you remember this news coming out?

00:23:45.107 --> 00:23:46.967
How did you feel when it did?

00:23:47.717 --> 00:23:51.737
If you are a Facebook user, or I should probably say Meta now,

00:23:51.737 --> 00:23:55.837
shouldn’t I, anyway, how did it affect your behaviour, if at all?

00:23:56.357 --> 00:23:59.987
I would love to know, so let’s get this discussion started.

00:24:00.447 --> 00:24:03.207
You can head right into our community forum, which is at

00:24:03.207 --> 00:24:08.227
community.leonardoenglish.com and get chatting away to other curious minds.

00:24:09.207 --> 00:24:13.937
You've been listening to English Learning for Curious Minds, by Leonardo English.

00:24:14.307 --> 00:24:18.627
I'm Alastair Budge, you stay safe, and I'll catch you in the next episode.

