WEBVTT

NOTE
This file was generated by Descript <www.descript.com>

00:00:05.130 --> 00:00:11.930
Hello, hello hello, and welcome to English Learning for Curious Minds, by Leonardo English.

00:00:12.250 --> 00:00:16.650
The show where you can listen to fascinating stories, and learn weird and

00:00:16.650 --> 00:00:20.729
wonderful things about the world at the same time as improving your English.

00:00:21.400 --> 00:00:27.630
I'm Alastair Budge, and today we are going to be talking about something called Effective Altruism.

00:00:28.290 --> 00:00:32.989
It’s an idea and a way of life that is all about doing good, helping

00:00:33.010 --> 00:00:36.799
others and improving the world, in the most efficient way possible.

00:00:37.410 --> 00:00:41.980
It’s a super interesting idea, and I’m really excited to tell you more about it today.

00:00:43.014 --> 00:00:45.764
OK then, Effective Altruism.

00:00:47.274 --> 00:00:51.094
Let me start this episode with a theoretical situation.

00:00:52.354 --> 00:00:58.734
Imagine that it is a lovely day, and you are walking through a park in your town or city.

00:00:59.484 --> 00:01:04.214
The sun is shining, the birds are singing, it’s a Saturday

00:01:04.214 --> 00:01:07.314
morning and you have the whole weekend ahead of you.

00:01:08.454 --> 00:01:12.524
As you turn the corner to the duck pond, you see a young

00:01:12.564 --> 00:01:16.414
child crying out for help in the middle of the pond.

00:01:16.994 --> 00:01:20.694
The child must have fallen in, they clearly cannot

00:01:20.694 --> 00:01:24.254
swim, and there is nobody else around to help them.

00:01:25.804 --> 00:01:30.524
The pond isn’t very deep, and what’s more, you are a confident swimmer.

00:01:31.564 --> 00:01:36.894
Diving in to save the child will mean that your clothes will get wet and you’ll need to

00:01:36.894 --> 00:01:43.174
change them before the cinema that afternoon, but it will only be mildly inconvenient.

00:01:44.204 --> 00:01:46.514
You have an important choice to make.

00:01:47.414 --> 00:01:51.254
If you dive in, you will save the child’s life.

00:01:52.024 --> 00:01:54.754
If you do nothing, the child will die.

00:01:55.944 --> 00:01:56.684
What do you do?

00:01:57.964 --> 00:02:01.734
Now, I’d be very surprised if anyone was sitting there listening to this

00:02:02.024 --> 00:02:05.934
and thinking, “I’ll just keep on going, that child means nothing to me”.

00:02:06.634 --> 00:02:09.414
Of course, you dive in and save the child.

00:02:10.004 --> 00:02:13.624
A minor inconvenience to you, in the form of wet clothes,

00:02:13.894 --> 00:02:18.224
is far outweighed by the saving of a child’s life.

00:02:19.564 --> 00:02:24.904
But, to take this thought experiment one step further, what if you

00:02:24.904 --> 00:02:29.094
cannot see the child; they might be several thousand kilometres away.

00:02:29.958 --> 00:02:34.164
And you wouldn’t be saving them personally, you wouldn’t be

00:02:34.164 --> 00:02:37.474
diving into a pond or river, or doing anything like that.

00:02:38.634 --> 00:02:44.274
Instead, you would have to endure a mild inconvenience in the form of giving away the money

00:02:44.374 --> 00:02:49.204
that you were planning to spend on a cinema ticket in order to save an unknown child’s life.

00:02:50.494 --> 00:02:55.414
Now, perhaps it gets slightly more complicated, but I’m sure that most people

00:02:55.414 --> 00:03:00.684
would say “yes, if you told me with absolute certainty that by giving up my

00:03:00.684 --> 00:03:05.374
cinema ticket this afternoon, a child’s life would be saved, yes I would do it”.

00:03:07.084 --> 00:03:10.244
But let’s take it one step further again.

00:03:11.474 --> 00:03:18.184
Imagine that you needed to give up, let’s say, 10% of your annual income, the money that you might

00:03:18.184 --> 00:03:24.464
spend every year for things like cinema tickets or holidays or general nice things in your life.

00:03:25.354 --> 00:03:29.094
In exchange, the lives of some children might be saved.

00:03:30.194 --> 00:03:30.894
Would you do it?

00:03:31.874 --> 00:03:34.594
Now how about 30% of your income?

00:03:35.294 --> 00:03:35.964
50%?

00:03:36.343 --> 00:03:37.164
90%?

00:03:38.614 --> 00:03:43.334
This, at least the first example of the drowning child, is one of the

00:03:43.364 --> 00:03:47.964
core arguments of the most influential essays in Effective Altruism.

00:03:48.784 --> 00:03:53.624
It was called Famine, Affluence, and Morality, and was written

00:03:53.724 --> 00:03:56.994
by an Australian moral philosopher called Peter Singer.

00:03:58.214 --> 00:04:03.994
In it, Singer writes, and I’m quoting directly: “It makes no moral difference

00:04:04.204 --> 00:04:08.614
whether the person I can help is a neighbour's child ten yards away from

00:04:08.614 --> 00:04:14.014
me or a Bengali whose name I shall never know, ten thousand miles away...“

00:04:15.554 --> 00:04:22.384
In other words, a life is a life, and if you can do something to help, you should do it.

00:04:23.294 --> 00:04:26.824
We know that people are suffering and their lives are at threat,

00:04:27.244 --> 00:04:31.794
and if we can, we have a moral obligation to do something about it.

00:04:33.364 --> 00:04:38.774
This essay was published in 1972, and is considered to be one

00:04:38.784 --> 00:04:43.114
of the most important essays in 20th century moral philosophy.

00:04:44.494 --> 00:04:50.644
This essay was well-received when it was published, but it wasn’t until 2005,

00:04:51.124 --> 00:04:56.524
more than 30 years later, that it would have a particularly important reader.

00:04:58.084 --> 00:05:03.344
The reader was an 18-year-old Scottish man by the name of William MacAskill.

00:05:04.464 --> 00:05:10.144
Now, at the time he was an undergraduate philosophy student at Cambridge University,

00:05:10.754 --> 00:05:16.524
but Singer’s writings struck such a chord with him that he would go on to be one of

00:05:16.524 --> 00:05:22.244
the founders of a movement called Effective Altruism, the subject of today’s episode.

00:05:23.404 --> 00:05:28.324
Now, you’ve heard a little bit about what Effective Altruism is, but

00:05:28.584 --> 00:05:33.224
let me read you out the official explanation from its own website:

00:05:34.684 --> 00:05:38.364
“Effective altruism is a project that aims to find the

00:05:38.374 --> 00:05:42.004
best ways to help others, and put them into practice.

00:05:42.854 --> 00:05:48.904
It’s both a research field, which aims to identify the world’s most pressing problems and the

00:05:48.904 --> 00:05:55.764
best solutions to them, and a practical community that aims to use those findings to do good.

00:05:56.794 --> 00:06:03.544
This project matters because, while many attempts to do good fail, some are enormously effective.

00:06:04.364 --> 00:06:09.564
For instance, some charities help 100 or even 1,000 times as

00:06:09.564 --> 00:06:13.454
many people as others, when given the same amount of resources.

00:06:14.654 --> 00:06:18.834
This means that by thinking carefully about the best ways to

00:06:18.834 --> 00:06:23.284
help, we can do far more to tackle the world’s biggest problems.“

00:06:24.134 --> 00:06:28.354
In other words, the goal of effective altruism is not just to

00:06:28.404 --> 00:06:33.554
improve the world, but to do this in the most efficient way possible.

00:06:35.084 --> 00:06:39.814
The movement is particularly focussed on attracting often very clever young

00:06:39.814 --> 00:06:44.784
people, people who might go on to great success in whatever field they want,

00:06:45.254 --> 00:06:51.124
and encouraging them to become an Effective Altruist, or an EA for short.

00:06:51.984 --> 00:06:54.724
So, what does this mean in practice?

00:06:55.274 --> 00:06:58.944
How do you have the greatest possible impact on humanity?

00:06:59.444 --> 00:07:01.124
How can you do the most good?

00:07:01.764 --> 00:07:06.004
How can you use the time that you have on this Earth most effectively?

00:07:07.534 --> 00:07:13.624
It is, according to Effective Altruism, a question of mathematics and statistics.

00:07:15.104 --> 00:07:20.174
You might think that the automatic reaction for someone who wants to “do good”

00:07:20.444 --> 00:07:24.634
in the world might be something like becoming a doctor in a rural village in

00:07:24.634 --> 00:07:30.574
India or by digging wells in Kenya, where they directly help people who need it.

00:07:32.074 --> 00:07:36.384
According to Effective Altruism, these might be good things to

00:07:36.384 --> 00:07:41.164
do, but they are typically not the most effective things to do.

00:07:42.654 --> 00:07:49.524
Instead, a person should look at how to maximise their impact; how not only to work on

00:07:49.524 --> 00:07:55.054
the most important problems, but how to solve these for the greatest number of people.

00:07:56.374 --> 00:08:01.484
This could be in groundbreaking scientific or medical research, where you might discover

00:08:01.484 --> 00:08:05.744
something that can impact the lives of hundreds of millions, or billions of people.

00:08:06.484 --> 00:08:12.144
It might be by entering politics, where you can influence policy and direct government

00:08:12.154 --> 00:08:17.314
money towards “good” causes, in a way that you could never do as an individual actor.

00:08:18.244 --> 00:08:23.544
It could be by convincing other people to become effective altruists, as if you

00:08:23.544 --> 00:08:28.434
can convince multiple people to become EAs, then your impact will be multiplied.

00:08:29.394 --> 00:08:35.134
It could even be by joining a bank or hedge fund and earning vast amounts of money.

00:08:36.574 --> 00:08:42.634
This might sound like a strange way to “do good”, but according to Effective

00:08:42.664 --> 00:08:48.264
Altruism, earning huge amounts of money can be one of the best possible ways to help.

00:08:49.144 --> 00:08:51.544
Providing, that is, that you give it away.

00:08:53.087 --> 00:08:56.667
Let me illustrate this with two different examples.

00:08:57.947 --> 00:09:02.237
Imagine that there are two twins, a brother and a sister.

00:09:03.097 --> 00:09:05.827
They are both very intelligent, they have been to the

00:09:05.837 --> 00:09:10.187
best universities and can go on to any career they want.

00:09:11.713 --> 00:09:17.423
The brother decides to become a doctor and help rural children in Bangladesh.

00:09:18.363 --> 00:09:23.873
He saves the lives of 100 children per year personally, with his own hands.

00:09:25.243 --> 00:09:28.043
Now his sister takes a different path.

00:09:29.023 --> 00:09:33.123
She takes a job with a top financial company in New York City.

00:09:34.223 --> 00:09:38.463
She doevery s well at her job, and before long she is making

00:09:38.523 --> 00:09:42.053
$10 million dollars a year, sometimes more on a good year.

00:09:43.593 --> 00:09:48.993
After taxes and living expenses, let’s say she gives away $5 million

00:09:48.993 --> 00:09:54.353
dollars a year to organisations with a proven track record of efficiently

00:09:54.353 --> 00:09:58.543
training rural doctors in the same region her brother is working in.

00:09:59.903 --> 00:10:06.383
Let’s say, for sake of ease, it costs $50,000 to train a doctor in Bangladesh,

00:10:06.723 --> 00:10:11.288
meaning that the sister’s donation can train 100 new doctors every year.

00:10:12.148 --> 00:10:15.998
And let’s assume that each new doctor is as efficient as

00:10:15.998 --> 00:10:20.408
the brother, so can save the lives of 100 children per year.

00:10:21.678 --> 00:10:22.668
Let’s do the sums then.

00:10:23.428 --> 00:10:27.018
The brother has gone out and become a doctor himself, and

00:10:27.018 --> 00:10:30.908
is responsible for 100 children’s lives saved every year.

00:10:32.028 --> 00:10:36.588
Certainly, an admirable achievement, well done him, but let’s

00:10:36.608 --> 00:10:40.468
run the numbers for his twin sister who went to work in finance.

00:10:41.798 --> 00:10:46.688
Her job allows her to pay for 100 new doctors to be trained every year.

00:10:47.648 --> 00:10:53.178
After 10 years, she has paid for the training of 1,000 new doctors, each

00:10:53.198 --> 00:11:00.233
of whom saves 100 children per year, so her contribution has led to 100,000

00:11:00.298 --> 00:11:05.095
children’s lives being saved every year, while her brother is still saving 100.

00:11:06.758 --> 00:11:10.858
Now, the numbers might be slightly off, and of course there might not be the

00:11:10.858 --> 00:11:15.828
need for an additional 1,000 doctors in that region, but you see what I mean.

00:11:17.178 --> 00:11:22.788
The choice proposed by Effective Altruism is not to do the thing that seems obvious,

00:11:23.118 --> 00:11:28.568
to go and work in the field, directly “saving lives”, but to look at the skills and

00:11:28.568 --> 00:11:34.218
opportunities that you possess and use those skills to have the greatest possible impact.

00:11:35.348 --> 00:11:40.408
As we saw with the example of the sister, sometimes the most efficient way to have

00:11:40.408 --> 00:11:45.728
an impact is to earn lots of money to pay for others to do the hands-on work for you.

00:11:47.268 --> 00:11:52.568
And this kind of, perhaps obvious, humanitarian work is just one

00:11:52.578 --> 00:11:57.498
example of a life being “directly” saved, and impact being delivered.

00:11:58.568 --> 00:12:03.588
The Effective Altruism website has a bunch of different examples that demonstrate

00:12:03.598 --> 00:12:08.378
Effective Altruism in practice, how to actually have an outsized impact.

00:12:09.748 --> 00:12:14.348
These typically involve areas which have not historically attracted much

00:12:14.368 --> 00:12:20.778
attention or funding, but could have a huge positive impact on society.

00:12:22.098 --> 00:12:25.378
The first example on the website is to do with preventing

00:12:25.388 --> 00:12:29.020
pandemics, and comparing this to global terrorism.

00:12:30.490 --> 00:12:33.896
Statistically speaking, in terms of the number of global

00:12:33.906 --> 00:12:39.116
deaths, terrorism is a very minor problem compared to pandemics.

00:12:40.566 --> 00:12:46.236
In the last 50 years, fewer than 500,000 people have been killed in

00:12:46.236 --> 00:12:51.716
terrorist attacks, while over 21 million people died in COVID-19 alone.

00:12:52.646 --> 00:12:58.386
Yet the US government spends $280 billion per year on

00:12:58.416 --> 00:13:02.936
counterterrorism but only $8 billion on pandemic prevention.

00:13:04.448 --> 00:13:09.108
Put another way, despite the fact that more than 40 times more people

00:13:09.348 --> 00:13:13.208
died from the last pandemic than have died in terrorist attacks in

00:13:13.208 --> 00:13:18.569
the last 50 years, pandemic research gets less than 3% of the funding.

00:13:19.859 --> 00:13:23.789
In the eyes of Effective Altruism, this makes little sense.

00:13:24.499 --> 00:13:28.185
Yes, preventing terrorist attacks is important, but

00:13:28.672 --> 00:13:32.599
pandemics seem like such a greater threat to human wellbeing.

00:13:34.009 --> 00:13:40.289
Or let me give you another example, if you believe that animals deserve a dignified and happy

00:13:40.289 --> 00:13:46.299
life, not to suffer, like Effective Altruists do, where should you focus your attention?

00:13:47.233 --> 00:13:51.213
Animal shelters, which provide a home for unwanted pets?

00:13:51.813 --> 00:13:57.503
Or on pushing for systemic change to end the practice of factory farming of animals?

00:13:58.693 --> 00:14:03.783
Again, with our good Effective Altruist hat on, let’s look at the numbers to

00:14:03.803 --> 00:14:09.263
evaluate the scale of the problem, and therefore the potential impact by solving it.

00:14:10.843 --> 00:14:14.873
In the United States, there are 7 million animals in animal

00:14:14.873 --> 00:14:20.173
shelters, but over 10 billion animals in factory farms.

00:14:20.663 --> 00:14:26.073
10 billion, not 10 million, so there are 1,400 times more

00:14:26.113 --> 00:14:29.863
animals suffering in factory farms than in animal shelters.

00:14:31.023 --> 00:14:36.693
Animal shelters receive $5 billion per year in funding, but less than

00:14:36.693 --> 00:14:42.026
$100 million is spent every year on advocacy to end factory farming.

00:14:42.666 --> 00:14:47.346
Again, $5 billion with a b spent on animal shelters but

00:14:47.576 --> 00:14:51.486
only $100 million with an m on ending factory farming.

00:14:52.886 --> 00:14:59.876
With an EA mindset, you might look at this problem and think, ok, it seems like this is an area

00:15:00.026 --> 00:15:05.716
in which I can have a high level of impact, either by working directly on ending factory farming,

00:15:06.206 --> 00:15:11.156
working with a politician perhaps, or if you take the example of the sister we heard about

00:15:11.196 --> 00:15:18.086
earlier, earning lots of money and putting it towards ending factory farming advocacy activities.

00:15:19.916 --> 00:15:26.846
The point is, and this is one of the key beliefs of Effective Altruism, you should work on the area

00:15:26.976 --> 00:15:33.506
where you can have the greatest impact, not on one particular cause or impact that you might like.

00:15:35.046 --> 00:15:37.236
The movement has a term for this.

00:15:37.586 --> 00:15:42.946
It’s “open truth-seeking”, and it’s one of the core principles of the movement.

00:15:44.056 --> 00:15:49.666
Now, moving on to the people behind the movement, one of the most visible recent

00:15:49.706 --> 00:15:55.136
proponents of Effective Altruism is someone who has now been completely cast

00:15:55.166 --> 00:16:00.696
out from the movement, and that’s Sam Bankman-Fried, or SBF as he is known.

00:16:01.776 --> 00:16:06.526
You might know him as the boss of the former multibillion dollar crypto trading

00:16:06.526 --> 00:16:13.666
company, FTX, a man who was recently convicted of 7 counts of fraud and is, at the

00:16:13.676 --> 00:16:18.606
time of recording this episode at least, awaiting a very lengthy prison sentence.

00:16:19.686 --> 00:16:25.626
Now, the interesting thing about SBF in the context of this episode is that he wasn’t

00:16:25.636 --> 00:16:31.866
some multi-billionaire who then decided he wanted to give his money away; he was first

00:16:31.866 --> 00:16:39.566
and foremost an Effective Altruist, and FTX, his crypto business, was his way of earning

00:16:39.576 --> 00:16:44.476
enough money so that he could give it away to solve the world’s most pressing problems.

00:16:45.836 --> 00:16:51.336
In our twins example, he was the sister, apart from we would have to add on a few

00:16:51.346 --> 00:16:56.836
zeroes to the amount of money he was able to make, and how much he ended up giving away.

00:16:58.426 --> 00:17:03.716
It’s not clear exactly how much SBF gave away before he was arrested,

00:17:04.266 --> 00:17:09.256
but it is in the hundreds of millions of dollars, and he pledged to

00:17:09.256 --> 00:17:13.116
give away most of his multi-billion dollar fortune during his lifetime.

00:17:14.746 --> 00:17:20.066
In fact, according to a recent book about FTX by Michael Lewis, one of

00:17:20.106 --> 00:17:26.016
SBF’s biggest concerns was how to give his money away most effectively.

00:17:26.866 --> 00:17:32.526
He thought of everything in percentage terms, calculating the cost of something and its

00:17:32.546 --> 00:17:39.046
probable return, so he ended up giving millions to pandemic research groups, to political

00:17:39.046 --> 00:17:45.176
groups that would further policies that SBF supported, and to animal welfare organisations.

00:17:46.696 --> 00:17:53.356
He even, according to Michael Lewis, asked the Trump campaign how much it would cost

00:17:53.576 --> 00:18:00.246
for Donald Trump not to run in the next election, with SBF thinking that this could be a

00:18:00.256 --> 00:18:06.926
good use of his money if he removed what he saw as a danger for the country and humanity.

00:18:08.326 --> 00:18:13.426
Unfortunately the price, $5 billion proposed by the Trump campaign, was a

00:18:13.426 --> 00:18:18.626
little too high even for the billionaire SBF, and the deal never went through.

00:18:20.086 --> 00:18:27.046
Anyway, the reason to mention SBF was because he was, for many, the poster boy for Effective

00:18:27.076 --> 00:18:33.356
Altruism, he was the world’s best-known effective altruist, and his reason to get into crypto

00:18:33.686 --> 00:18:39.776
was a rational choice because it presented him with the best possible way to make a fortune.

00:18:40.736 --> 00:18:45.946
And this was all, at least if we take SBF at his word, with the

00:18:45.966 --> 00:18:50.566
express intention of giving it away to do good in the world.

00:18:51.929 --> 00:18:57.869
Now, taking SBF "at his word” might be a difficult thing to do now, given his crimes,

00:18:58.349 --> 00:19:05.099
but until his downfall, he was the embodiment of Effective Altruism, a man who publicly

00:19:05.129 --> 00:19:10.189
pledged to the world’s media that his mission was to make the world a better place.

00:19:11.749 --> 00:19:16.329
And getting involved in Effective Altruism certainly does not require

00:19:16.329 --> 00:19:21.739
you to make an SBF-style fortune, maximising your earnings solely

00:19:21.739 --> 00:19:24.959
in order to be able to give money away to effective charities.

00:19:25.989 --> 00:19:30.919
If you read the literature on Effective Altruism, it becomes clear that it’s

00:19:30.949 --> 00:19:35.779
actually a relatively liberal philosophy, in the traditional sense of the word.

00:19:36.679 --> 00:19:41.549
There are recommendations for what people should look into and what they should do.

00:19:42.339 --> 00:19:46.479
There’s even an affiliated group called “Giving What We Can”, where

00:19:46.639 --> 00:19:51.189
members are encouraged to give at least 10% of their income to good causes.

00:19:52.239 --> 00:19:54.389
But it is far from prescriptive.

00:19:55.079 --> 00:20:01.779
The general idea seems to be to attract people first to the concept of Effective Altruism,

00:20:02.149 --> 00:20:07.949
to show the disparity that often exists between minor problems that are treated as

00:20:07.959 --> 00:20:13.649
serious problems and serious potential problems that are treated as minor problems.

00:20:15.189 --> 00:20:21.359
And then, when someone sees this and subscribes to the overall philosophy and takes it to its

00:20:21.369 --> 00:20:26.649
logical conclusion, it brings them to a point where they look at the working hours that they

00:20:26.649 --> 00:20:32.869
might have in their life and think “how can I spend this time to do the most good in the world?”

00:20:33.652 --> 00:20:40.606
As we see from the very prominent example of Sam Bankman-Fried, Effective Altruism is certainly not

00:20:40.636 --> 00:20:47.276
without its contradictions and problems, but it is a unique and novel way of looking at the world.

00:20:47.836 --> 00:20:52.746
It’s an interesting lens through which someone starting out on their career can make a

00:20:52.746 --> 00:20:58.076
choice, but it’s also interesting for anyone thinking about “doing good” in the world.

00:20:59.096 --> 00:21:05.686
Peter Singer, the father of this type of moral philosophy, summed it up like this: “If we can

00:21:05.716 --> 00:21:11.576
prevent something bad, without sacrificing anything of comparable significance, we ought to do it.”

00:21:13.049 --> 00:21:19.149
Critics of Singer, and of Effective Altruism, would certainly argue that this is a simplistic

00:21:19.149 --> 00:21:24.229
way of viewing the world’s problems, that it puts too much responsibility on the shoulders of

00:21:24.229 --> 00:21:29.869
individuals, that it can get in the way of personal liberty and that it just isn’t practical.

00:21:31.393 --> 00:21:36.213
These might all be perfectly valid arguments, but the one final thought

00:21:36.403 --> 00:21:41.393
I’ll leave you with is that having even a small number of incredibly smart

00:21:41.393 --> 00:21:45.493
people thinking about solving the world’s most pressing problems in the most

00:21:45.503 --> 00:21:51.173
impactful way, well that certainly doesn’t seem like such a bad idea in my book.

00:21:53.773 --> 00:21:58.123
OK then, that is it for today's episode on Effective Altruism.

00:21:58.593 --> 00:22:01.803
I hope it's been an interesting one, and that you've learnt something new.

00:22:02.523 --> 00:22:05.503
As always, I would love to know what you thought of this episode.

00:22:05.963 --> 00:22:07.643
Are you an effective altruist?

00:22:08.043 --> 00:22:10.613
Had you heard of effective altruism before?

00:22:11.113 --> 00:22:12.093
What do you think about it?

00:22:12.733 --> 00:22:15.993
I would love to know, so let’s get this discussion started.

00:22:16.423 --> 00:22:18.993
You can head right into our community forum, which is at

00:22:18.993 --> 00:22:23.713
community.leonardoenglish.com and get chatting away to other curious minds.

00:22:24.233 --> 00:22:28.783
You've been listening to English Learning for Curious Minds, by Leonardo English.

00:22:29.243 --> 00:22:34.163
I'm Alastair Budge, you stay safe, and I'll catch you in the next episode.

