WEBVTT

NOTE
This file was generated by Descript <www.descript.com>

00:00:05.090 --> 00:00:12.170
Hello, hello hello, and welcome to English Learning for Curious Minds, by Leonardo English.

00:00:12.450 --> 00:00:16.790
The show where you can listen to fascinating stories, and learn weird and

00:00:16.799 --> 00:00:21.330
wonderful things about the world at the same time as improving your English.

00:00:22.209 --> 00:00:26.680
I'm Alastair Budge and today we are going to be talking about The

00:00:26.720 --> 00:00:31.720
Trolley Problem, the tough philosophical and ethical question that

00:00:31.759 --> 00:00:36.960
asks us to consider whether we would sacrifice a life to save another.

00:00:37.830 --> 00:00:42.070
You may well have heard of this problem before, but we are going to explore

00:00:42.150 --> 00:00:47.460
all of the different ideas around it, diving into the ethics and philosophy of

00:00:47.460 --> 00:00:52.780
how, and under what conditions, certain decisions can be considered acceptable.

00:00:53.560 --> 00:00:58.169
It might seem like a theoretical and unrealistic question, but we

00:00:58.170 --> 00:01:02.180
will see that it’s actually one that is very relevant to our lives,

00:01:02.210 --> 00:01:06.680
with the development of autonomous vehicles, of self-driving cars.

00:01:07.740 --> 00:01:10.939
I want to thank Carmine, a recent economics graduate

00:01:10.940 --> 00:01:13.519
from the University of Naples for this suggestion.

00:01:13.759 --> 00:01:16.380
It’s an awesome idea, and I hope you enjoy it.

00:01:17.320 --> 00:01:21.339
So, let’s not waste a minute, and get stuck in right away.

00:01:21.390 --> 00:01:25.739
As a quick administrative note, before we start, a

00:01:25.910 --> 00:01:30.720
trolley is another word for a tram, a one carriage train.

00:01:31.060 --> 00:01:37.119
Whether it’s a trolley, a tram or a train doesn’t really matter for the purposes of the problem

00:01:37.350 --> 00:01:42.869
- the point is that they are all big, heavy objects that will probably kill you if they hit you.

00:01:43.679 --> 00:01:49.250
There are several variants of the trolley problem, but they all go something like this.

00:01:50.290 --> 00:01:53.890
There is an out of control trolley going down the tracks.

00:01:54.230 --> 00:01:59.970
Ahead, on the tracks, there are five people tied up and unable to move.

00:02:00.580 --> 00:02:02.899
The trolley is headed straight for them.

00:02:03.560 --> 00:02:09.379
You are standing some distance off in the operating room, next to a lever.

00:02:10.510 --> 00:02:15.810
If you pull the lever, the trolley will switch to a different set of tracks.

00:02:16.960 --> 00:02:21.320
However, you notice that there is one person on this set of tracks.

00:02:21.820 --> 00:02:28.420
If you pull the lever, the trolley will move to the other set of tracks and kill the one person.

00:02:29.310 --> 00:02:31.020
So, you have two options.

00:02:31.630 --> 00:02:36.830
Do you do nothing and allow the trolley to kill the five people on the main track.

00:02:37.469 --> 00:02:44.229
Or do you pull the lever, diverting the trolley onto the other track where it will kill one person?

00:02:44.849 --> 00:02:47.339
What is the ethically correct option?

00:02:47.890 --> 00:02:51.560
Or, to put it simply, what is the right thing to do?

00:02:52.470 --> 00:02:56.790
Although there have been similar questions proposed for centuries,

00:02:56.980 --> 00:03:01.880
this question was really popularised by two women - an English woman

00:03:02.089 --> 00:03:06.543
called Philippa Foot, and an American called Judith Jarvis Thomson.

00:03:07.509 --> 00:03:14.410
Foot’s original version first involved a judge, before introducing the idea of the trolley, or tram.

00:03:15.350 --> 00:03:19.090
Her version proposed this hypothetical situation.

00:03:20.179 --> 00:03:27.390
Imagine that there is a trial for a particular crime, and a judge cannot decide who is guilty.

00:03:28.380 --> 00:03:31.050
They cannot find the person who committed the crime.

00:03:31.715 --> 00:03:36.700
Outside, some rioters are demanding that the judge finds

00:03:36.710 --> 00:03:40.809
someone guilty of the crime, and that person is put to death.

00:03:41.457 --> 00:03:46.830
If the judge doesn’t find someone guilty, these rioters, these protestors will

00:03:46.850 --> 00:03:52.019
take revenge on a particular section of the community, killing five people.

00:03:53.160 --> 00:03:59.030
Given that the judge can’t find the real guilty person, he decides

00:03:59.150 --> 00:04:03.560
that the only way he can avoid these five people being killed is by

00:04:03.590 --> 00:04:08.440
finding an innocent person and sentencing him to death for the crime.

00:04:09.259 --> 00:04:15.970
In this case, the judge decided to kill one innocent person to save a group of five innocent people.

00:04:16.779 --> 00:04:20.829
Now, Foot expanded on this and asked us to imagine

00:04:20.839 --> 00:04:24.540
a pilot in charge of a plane that is about to crash.

00:04:25.340 --> 00:04:29.489
The pilot can choose to crash into a less inhabited area,

00:04:29.650 --> 00:04:33.609
into an area with fewer houses, to kill fewer people.

00:04:34.299 --> 00:04:35.599
Should the pilot do it?

00:04:36.510 --> 00:04:41.840
Or, Foot proposed, what if instead of a plane it was a tram, a trolley, and

00:04:42.010 --> 00:04:46.739
the driver could flip a switch and kill only one person instead of five.

00:04:47.299 --> 00:04:48.849
What is the right thing to do?

00:04:49.770 --> 00:04:53.810
Now, in both cases, Foot showed, the result is the same.

00:04:53.960 --> 00:04:58.360
There is the exchange of one person’s life for five lives.

00:04:59.180 --> 00:05:04.539
But why is it that for most people they would say that they would flip the

00:05:04.539 --> 00:05:09.929
switch and allow the train to kill one person in order to save the five, when

00:05:10.119 --> 00:05:14.870
they wouldn’t agree that the judge did the ethically correct thing by finding

00:05:14.870 --> 00:05:19.659
an innocent person and sentencing them to death, to save the five other people?

00:05:20.708 --> 00:05:25.809
Since the original publication of this article, in 1967, there have

00:05:25.830 --> 00:05:30.050
been multiple developments and variants on this problem, which ask

00:05:30.070 --> 00:05:35.049
us to consider how our views change depending on the circumstances.

00:05:35.820 --> 00:05:43.590
For example, in 1976 Judith Jarvis Thomson proposed an alternative with a surgeon, with a doctor.

00:05:44.380 --> 00:05:51.460
Imagine that there is a brilliant surgeon with five patients, each in need of a different organ.

00:05:52.280 --> 00:05:57.660
One needs a heart, another needs a new lung, another, a new liver,

00:05:58.200 --> 00:06:02.999
another, new kidneys, and the final one needs a new stomach.

00:06:03.879 --> 00:06:06.789
Each of whom will die without that organ.

00:06:07.859 --> 00:06:14.440
Unfortunately, no suitable organs are available to perform any of these five transplant operations.

00:06:15.720 --> 00:06:30.294
A healthy young traveller, just passing through the city in which the doctor works, comes in for a

00:06:30.294 --> 00:06:30.310
routine checkup, there's nothing wrong with him.

00:06:30.310 --> 00:06:30.314
In the

00:06:30.314 --> 00:06:30.369
course of doing the checkup, the doctor discovers that this young

00:06:30.369 --> 00:06:34.700
traveller’s organs are compatible with all five of his dying patients.

00:06:35.799 --> 00:06:39.570
Suppose further that if the young man were to disappear,

00:06:39.900 --> 00:06:42.939
no one would suspect the doctor, he wouldn't be caught.

00:06:43.909 --> 00:06:49.030
Do you support the morality of the doctor to kill that tourist and provide

00:06:49.030 --> 00:06:53.339
his healthy organs to those five dying people and save their lives?

00:06:54.099 --> 00:06:58.018
Again, the answer might be “probably not”, even though the

00:06:58.028 --> 00:07:02.048
result is the same, five people live and one person dies.

00:07:02.898 --> 00:07:06.168
There’s a similar version of this that is more similar

00:07:06.168 --> 00:07:10.144
to the original train problem and involves a fat man.

00:07:11.234 --> 00:07:16.638
Imagine that you are walking along a bridge and you can see a train running down the tracks.

00:07:17.108 --> 00:07:18.958
Ahead of it are five men.

00:07:19.558 --> 00:07:24.838
They can’t escape, and the train will hit and kill all of them, if you do nothing.

00:07:25.938 --> 00:07:29.498
Ahead of you, on the bridge is a very fat man.

00:07:30.528 --> 00:07:37.098
You know that you can push him over the bridge onto the train tracks, and he will stop the train.

00:07:37.958 --> 00:07:42.058
The fat man will die, but the five men will be saved.

00:07:42.738 --> 00:07:43.628
What should you do?

00:07:44.488 --> 00:07:48.378
Is it different because you are actively killing a person

00:07:48.378 --> 00:07:52.928
to save five, instead of saving five to allow one to die?

00:07:54.048 --> 00:07:59.132
Other variants of this problem complicate it further by including emotion.

00:07:59.852 --> 00:08:05.252
Let’s say that instead of that one person on the railway tracks being a random person you don’t

00:08:05.262 --> 00:08:12.112
know, what if they were your son, daughter, husband, wife, brother, sister, mother or father?

00:08:12.702 --> 00:08:16.242
Or what if they were someone that you knew was evil,

00:08:16.332 --> 00:08:19.092
that was the complete opposite to someone close to you?

00:08:19.752 --> 00:08:24.612
How would your decision-making process change, given these differences?

00:08:25.552 --> 00:08:29.002
Obviously, there are no right or wrong answers here, only

00:08:29.162 --> 00:08:32.472
moral judgments of what we believe to be right and wrong.

00:08:33.112 --> 00:08:37.302
Is there a difference between actively killing someone and allowing

00:08:37.302 --> 00:08:40.732
someone to die, if that was what was going to happen anyway?

00:08:41.792 --> 00:08:48.082
A utilitarian view, which you can learn more about in episode 116 on Jeremy Bentham,

00:08:48.272 --> 00:08:53.612
tells us that we should flip the switch and allow the one person to die in order

00:08:53.622 --> 00:08:58.742
to save the five, because this is what causes the greatest happiness overall.

00:08:59.222 --> 00:09:01.312
Five lives are worth more than one.

00:09:02.552 --> 00:09:06.722
Not only would that decision be allowed by a utilitarian,

00:09:06.722 --> 00:09:10.252
but it would also be a morally better choice.

00:09:11.422 --> 00:09:16.122
There is, of course, an alternative view that merely participating in

00:09:16.122 --> 00:09:20.502
something that will result in the death of one person is morally wrong.

00:09:21.372 --> 00:09:26.852
The view goes that it isn’t your fault that the train is running down the tracks, you aren’t

00:09:26.892 --> 00:09:33.462
responsible for the death of those five people, and if you flipped the switch to allow the train

00:09:33.462 --> 00:09:39.872
to kill the one person instead, well you would take some responsibility for that person’s death.

00:09:40.932 --> 00:09:45.912
If this is how you would think about the situation, let me put a spanner in

00:09:45.912 --> 00:09:51.052
the works, let me complicate it further by proposing to you an alternative.

00:09:51.672 --> 00:09:54.652
Imagine that there was a train running down the tracks.

00:09:54.902 --> 00:09:59.442
You could see that it was on course to hit five people, killing them all.

00:09:59.982 --> 00:10:06.552
You could press a button and move the train to another track, where there was only one person.

00:10:07.032 --> 00:10:11.362
Instead of killing five people, only one person would be killed.

00:10:12.002 --> 00:10:17.732
But there was also another button where you could switch the train to another track altogether.

00:10:18.432 --> 00:10:21.522
This track was empty, and nobody would be killed.

00:10:22.192 --> 00:10:25.772
I imagine you would say that the correct course of action would

00:10:25.782 --> 00:10:29.947
be to move the train to the empty track and save everyone’s lives.

00:10:30.597 --> 00:10:31.347
Of course it is.

00:10:32.067 --> 00:10:36.537
But the point is that in this case, you have involved yourself with the

00:10:36.537 --> 00:10:40.987
situation, you have changed the natural course of what was going to happen,

00:10:41.377 --> 00:10:46.167
so if you would do it to save five lives, why wouldn’t you do it to save

00:10:46.317 --> 00:10:51.457
four lives, why wouldn’t you save the five people and only allow one to die?

00:10:52.457 --> 00:10:56.255
Now, the trolley problem, or the train problem, has come under

00:10:56.285 --> 00:11:00.755
a lot of scrutiny, and there are plenty of criticisms of it.

00:11:01.535 --> 00:11:03.215
Of course, it’s unrealistic.

00:11:03.405 --> 00:11:04.615
It’s theoretical.

00:11:04.935 --> 00:11:11.380
There are no situations in which we know exactly, in which we know with certainty, what will happen.

00:11:11.820 --> 00:11:16.010
So asking us to make these moral and ethical choices in

00:11:16.010 --> 00:11:21.030
completely certain situations is unrealistic, and not even useful.

00:11:21.890 --> 00:11:27.530
There’s also the point that most moral judgments do not involve life and death, thankfully.

00:11:28.540 --> 00:11:34.390
The trolley problem is too extreme, too unrealistic, and therefore it’s not actually

00:11:34.500 --> 00:11:39.570
helpful when thinking about moral or ethical decisions, so the criticism goes.

00:11:40.770 --> 00:11:46.820
In real life, there are very few times where anyone needs to make these kinds of decisions, and

00:11:46.820 --> 00:11:54.600
focusing on this kind of question deflects attention away from, it moves the focus away from more

00:11:54.610 --> 00:12:00.690
important, more realistic ethical and moral questions that we should spend more time thinking about.

00:12:01.610 --> 00:12:07.080
Finally, and perhaps most importantly, it reduces human life to a number.

00:12:08.090 --> 00:12:12.860
Of course, 5 is greater than 1, but that’s not how life works.

00:12:13.230 --> 00:12:20.080
Real-life has real people, they are different, humans make decisions in different ways, and to

00:12:20.080 --> 00:12:27.280
reduce the entire problem to arithmetic, to an algorithmic calculation, isn’t how life works.

00:12:28.120 --> 00:12:32.030
It makes you think of the famous quote that was reportedly said by

00:12:32.060 --> 00:12:36.920
Josef Stalin: “One death is a tragedy, a million deaths a statistic”.

00:12:37.443 --> 00:12:42.283
And Josef Stalin, of course, isn’t one of history’s great moral philosophers.

00:12:43.013 --> 00:12:47.173
Yet the trolley problem is again becoming increasingly relevant.

00:12:47.713 --> 00:12:50.723
But this time, we aren’t asking humans to make moral

00:12:50.723 --> 00:12:55.183
judgments, we are asking machines, we are asking algorithms.

00:12:56.153 --> 00:12:59.343
Of course, these algorithms need to be told what to

00:12:59.343 --> 00:13:02.823
do, they need to be given instructions by humans.

00:13:03.403 --> 00:13:07.823
I’m talking here about autonomous vehicles, about self-driving cars.

00:13:08.513 --> 00:13:13.373
With a human driver, we rely on humans to make decisions about what to do.

00:13:13.933 --> 00:13:18.853
If a pedestrian steps out into the road unexpectedly, the driver

00:13:18.863 --> 00:13:23.233
would swerve, they would move quickly to try to avoid them.

00:13:23.833 --> 00:13:28.423
If there were a situation in which your car was out of control, and you had

00:13:28.433 --> 00:13:33.343
to choose between hitting a group of people and swerving hard to the right

00:13:33.543 --> 00:13:38.383
and you would only hit one person, at the moment you have to make that choice.

00:13:39.073 --> 00:13:42.023
Thankfully, it isn’t a choice that most of us will

00:13:42.023 --> 00:13:45.013
ever have to make, but still, it is a possibility.

00:13:45.733 --> 00:13:48.863
With self-driving cars, they drive themselves.

00:13:49.123 --> 00:13:54.843
Complex algorithms, complex computer code tells them what to do in certain situations.

00:13:55.503 --> 00:14:01.920
For the vast majority of the time, these are engineering decisions, not moral decisions.

00:14:02.663 --> 00:14:06.903
For example, if a self-driving car sees a stopped car ahead

00:14:06.903 --> 00:14:10.633
of it, it should slow down or move to the right or left.

00:14:11.073 --> 00:14:15.188
If there is a ball that bounces into the street, it should stop.

00:14:15.838 --> 00:14:19.508
From a technological point of view, of course, this is amazing,

00:14:19.678 --> 00:14:23.798
but from a moral point of view, there isn’t a huge amount going on.

00:14:24.268 --> 00:14:30.478
You don’t need to make moral decisions about turning left or right or slowing down in certain areas.

00:14:31.188 --> 00:14:34.808
But, what happens in a situation where a car does have to

00:14:34.808 --> 00:14:38.358
make a choice about where to cause the least amount of harm?

00:14:39.218 --> 00:14:43.028
Let’s return to our situation of either going straight

00:14:43.178 --> 00:14:47.328
into a crowd of people or swerving to only hit one person.

00:14:48.168 --> 00:14:49.918
What should the car do?

00:14:51.415 --> 00:14:57.855
Obviously, the software cannot foresee every potential situation and tell the car what to do

00:14:57.955 --> 00:15:04.335
every time, but it does need to provide a framework for the car to make decisions on its own.

00:15:05.105 --> 00:15:08.225
The Trolley Problem is therefore a useful way of

00:15:08.295 --> 00:15:11.775
thinking about this, but it’s, of course, imperfect.

00:15:12.595 --> 00:15:15.595
It gets even more complicated as we think about the

00:15:15.615 --> 00:15:19.585
implications of software making moral or ethical decisions.

00:15:20.205 --> 00:15:24.635
Let’s say that a self-driving car saw a motorbike ahead that was

00:15:24.645 --> 00:15:28.425
out of control and about to crash into a group of pedestrians.

00:15:29.055 --> 00:15:34.325
The self-driving car could brake and stop, thereby avoiding a crash with the motorbike,

00:15:34.595 --> 00:15:39.175
but the motorbike would crash into the pedestrians, seriously injuring or killing them.

00:15:39.715 --> 00:15:44.525
Or, the self-driving car could speed up, hitting the motorbike and

00:15:44.525 --> 00:15:48.685
probably killing its driver, but saving the group of pedestrians?

00:15:49.375 --> 00:15:50.235
What should it do?

00:15:51.455 --> 00:15:55.205
You might say, well, it should just brake because the actions of

00:15:55.205 --> 00:15:59.128
the motorbike aren’t its responsibility, or you could argue that

00:15:59.375 --> 00:16:03.365
it has a moral responsibility to save the pedestrians if it can.

00:16:04.545 --> 00:16:06.105
How about a different situation?

00:16:06.375 --> 00:16:11.575
Let’s say that a self-driving car with you inside was driving over a single-lane

00:16:11.595 --> 00:16:17.476
bridge and there was a group of 10 schoolchildren that had stepped into the road ahead.

00:16:18.415 --> 00:16:23.425
The car hadn’t seen them, they had stepped out quickly, and after doing all of the

00:16:23.445 --> 00:16:28.825
necessary calculations in a millisecond, the car knew that there wasn’t enough time

00:16:28.835 --> 00:16:33.415
for the children to move, and there wasn’t enough time for the car to slow down.

00:16:34.185 --> 00:16:39.075
But, it could swerve to the right and throw itself off the bridge, thereby

00:16:39.115 --> 00:16:43.245
killing you, the passenger, but saving the group of schoolchildren.

00:16:44.135 --> 00:16:46.495
What is the right thing for the car to do?

00:16:47.635 --> 00:16:53.045
Understandably, most people wouldn’t like the idea that their car could kill them in order

00:16:53.045 --> 00:16:58.775
to save complete strangers, and it is probably a strange idea for you to think that your car

00:16:58.985 --> 00:17:04.295
should be making moral judgments on your behalf, and sacrificing your life in the process.

00:17:04.995 --> 00:17:09.235
Should you, as a car owner, or as a passenger in a self-driving

00:17:09.235 --> 00:17:13.805
car, be able to select the ethics and morals of your car?

00:17:14.385 --> 00:17:17.205
Could you choose to have a selfless car that would

00:17:17.205 --> 00:17:20.835
sacrifice itself, and the people inside, in an instant?

00:17:21.575 --> 00:17:25.985
Or would you prefer to have a selfish car, which put much more

00:17:25.985 --> 00:17:29.835
value on the life of its passengers than any humans nearby?

00:17:30.655 --> 00:17:33.965
Going one step further, could you say that you absolutely loved

00:17:33.975 --> 00:17:38.115
kids, and you’d do anything to save anyone under the age of 12?

00:17:39.045 --> 00:17:44.205
But you hated animals and people over the age of 70 and were very

00:17:44.335 --> 00:17:48.005
happy to hit as many rabbits and old-age pensioners as possible.

00:17:49.005 --> 00:17:53.575
Obviously, the last point is an exaggeration, but the point is that the

00:17:53.575 --> 00:17:58.525
software in self-driving cars needs to be provided guidance on what to do by

00:17:58.535 --> 00:18:03.325
humans, we need to give them instructions for what to do in these situations.

00:18:03.875 --> 00:18:10.085
This software has complex machine-learning algorithms, so it does get smarter over time,

00:18:10.475 --> 00:18:16.365
but we can’t rely on the algorithms to make moral or ethical judgements on their own.

00:18:17.425 --> 00:18:22.825
Going back to our earlier criticism of the trolley problem, that it was too black and white,

00:18:23.005 --> 00:18:28.545
that it was either life or death, and it didn’t consider the fact that nothing was certain,

00:18:28.965 --> 00:18:34.905
computers are generally much better than humans at processing large amounts of information quickly.

00:18:35.985 --> 00:18:40.305
If you need to calculate the probability of certain things happening, the

00:18:40.515 --> 00:18:45.555
probability of being able to stop in time, the probability of death or serious

00:18:45.585 --> 00:18:50.605
injury in a certain type of collision, or similar complicated calculations,

00:18:50.945 --> 00:18:55.865
a computer is infinitely better and quicker at doing this than a human is.

00:18:56.505 --> 00:18:58.395
But, of course, it’s very complicated.

00:18:58.805 --> 00:19:00.355
And who decides?

00:19:00.835 --> 00:19:04.475
Is it left to the technology companies building the software,

00:19:04.655 --> 00:19:07.775
writing the algorithms to decide what is right or wrong?

00:19:08.525 --> 00:19:10.665
Or should it be a legal matter?

00:19:11.105 --> 00:19:15.645
Should the law of each country specify how autonomous cars should behave?

00:19:16.725 --> 00:19:20.165
And if so, how would this be different from country to country?

00:19:20.535 --> 00:19:25.455
Would the software have to be adjusted based on the country the car was registered in?

00:19:25.875 --> 00:19:28.695
Or would it be adjusted based on where the car was?

00:19:29.285 --> 00:19:34.215
Would you have a situation where you were in a self-driving car, and when it crossed

00:19:34.235 --> 00:19:39.695
national borders its software would automatically update to the moral code of the country?

00:19:40.755 --> 00:19:46.025
It does seem that, although the technology behind self-driving cars isn’t so far away,

00:19:46.245 --> 00:19:51.555
there still isn’t complete agreement on how they should behave from a moral point of view.

00:19:52.475 --> 00:19:54.475
And this is quite telling.

00:19:54.815 --> 00:19:57.925
These moral questions are ones that humans have been

00:19:57.945 --> 00:20:01.305
battling with for millennia, for thousands of years.

00:20:02.005 --> 00:20:05.885
The technology behind self-driving cars, although it is brilliant,

00:20:06.165 --> 00:20:10.385
is relatively new, and hasn’t taken that long to develop.

00:20:11.085 --> 00:20:15.805
Yet it looks like the software will arrive before agreement on the ethics.

00:20:16.235 --> 00:20:21.375
And that, for me, is a good indication of what the harder problem to solve might be.

00:20:24.475 --> 00:20:30.285
OK then, that is it for this exploration of the Trolley Problem, what it is, how it

00:20:30.285 --> 00:20:34.865
makes us think about our relationship with each other, and its relevance for us today

00:20:35.585 --> 00:20:38.845
I hope it's been an interesting one, and that you've learnt something new.

00:20:39.715 --> 00:20:42.375
You will note that I have not, or I have at least

00:20:42.545 --> 00:20:45.375
tried not to give any kind of moral judgments here.

00:20:45.625 --> 00:20:49.605
That is for you to decide, and there is clearly no right answer.

00:20:50.525 --> 00:20:53.545
I know we have lots of software developers who are members of

00:20:53.545 --> 00:20:56.935
Leonardo English, so what do you think of this moral problem?

00:20:57.295 --> 00:21:00.655
Where does the role of the programmer end, and where

00:21:00.655 --> 00:21:04.035
does the role of the lawmaker, or moralist, start?

00:21:04.705 --> 00:21:06.285
I would love to know what you think.

00:21:06.375 --> 00:21:09.255
You can head right into our community forum, which is at

00:21:09.255 --> 00:21:14.185
community.leonardoenglish.com and get chatting away to other curious minds.

00:21:15.395 --> 00:21:20.125
You've been listening to English Learning for Curious Minds, by Leonardo English.

00:21:20.525 --> 00:21:25.235
I'm Alastair Budge, you stay safe, and I'll catch you in the next episode.

